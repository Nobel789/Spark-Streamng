{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ba9a31f",
      "metadata": {
        "id": "9ba9a31f"
      },
      "source": [
        "# Streaming Prediction with Spark ML and Spark Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ea30cb",
      "metadata": {
        "id": "80ea30cb"
      },
      "source": [
        "In this notebook we are going to train a classification model to predict a patient's probability of suffering a heart attack."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating pipeline(a numbers of preprocess involved-assembler1, scaler, ohe, assembler2,lr ) and save it  for streaming data**"
      ],
      "metadata": {
        "id": "upRfoHyUtnsV"
      },
      "id": "upRfoHyUtnsV"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4_cxYJDgtbv",
        "outputId": "42f47d2f-e5a2-4213-a11d-6358b2a1f7b2"
      },
      "id": "i4_cxYJDgtbv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=02c71cf9d47bb122abae3a1f6da76f3f3637be9382f6ca0560c7251fd81a9750\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6318a507",
      "metadata": {
        "id": "6318a507"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4878544d",
      "metadata": {
        "id": "4878544d"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56cecab8",
      "metadata": {
        "id": "56cecab8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('UCI Heart disease').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe3c31c",
      "metadata": {
        "id": "afe3c31c",
        "outputId": "56396723-414f-4601-eb40-29b2247f932b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
            "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
            "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "heart = spark.read.csv(\"/content/heart.csv\",\n",
        "                       inferSchema = True,\n",
        "                       header = True)\n",
        "heart.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8216cc4",
      "metadata": {
        "id": "e8216cc4"
      },
      "outputs": [],
      "source": [
        "schema = StructType( \\\n",
        "                     [StructField(\"age\", LongType(),True), \\\n",
        "                      StructField(\"sex\", LongType(), True), \\\n",
        "                      StructField(\"cp\", LongType(), True), \\\n",
        "                      StructField('trestbps', LongType(), True), \\\n",
        "                      StructField(\"chol\", LongType(), True), \\\n",
        "                      StructField(\"fbs\", LongType(), True), \\\n",
        "                      StructField(\"restecg\", LongType(), True), \\\n",
        "                      StructField(\"thalach\", LongType(), True),\\\n",
        "                      StructField(\"exang\", LongType(), True), \\\n",
        "                      StructField(\"oldpeak\", DoubleType(), True), \\\n",
        "                      StructField(\"slope\", LongType(),True), \\\n",
        "                      StructField(\"ca\", LongType(), True), \\\n",
        "                      StructField(\"thal\", LongType(), True), \\\n",
        "                      StructField(\"target\", LongType(), True), \\\n",
        "                        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d8812f",
      "metadata": {
        "id": "28d8812f",
        "outputId": "ab9b6043-0ff4-42b8-9449-bb4e7c058137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- oldpeak: double (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- ca: integer (nullable = true)\n",
            " |-- thal: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import StructType,StructField,LongType, StringType,DoubleType,TimestampType\n",
        "\n",
        "df = heart.withColumnRenamed(\"target\",\"label\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2c3b4b",
      "metadata": {
        "id": "1b2c3b4b"
      },
      "outputs": [],
      "source": [
        "testDF, trainDF = df.randomSplit([0.3, 0.7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea2376e",
      "metadata": {
        "id": "9ea2376e"
      },
      "outputs": [],
      "source": [
        "# Create the logistic regression model\n",
        "lr = LogisticRegression(maxIter=10, regParam= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4007600c",
      "metadata": {
        "id": "4007600c"
      },
      "outputs": [],
      "source": [
        "# We create a one hot encoder.\n",
        "ohe = OneHotEncoder(inputCols = ['sex', 'cp', 'fbs', 'restecg', 'slope',\n",
        "                                 'exang', 'ca', 'thal'],\n",
        "                    outputCols=['sex_ohe', 'cp_ohe', 'fbs_ohe',\n",
        "                                'restecg_ohe', 'slp_ohe', 'exng_ohe',\n",
        "                                'caa_ohe', 'thall_ohe'])\n",
        "\n",
        "# Input list for scaling\n",
        "inputs = ['age','trestbps','chol','thalach','oldpeak']\n",
        "\n",
        "# We scale our inputs\n",
        "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
        "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n",
        "\n",
        "# We create a second assembler for the encoded columns.\n",
        "assembler2 = VectorAssembler(inputCols=['sex_ohe', 'cp_ohe',\n",
        "                                        'fbs_ohe', 'restecg_ohe',\n",
        "                                        'slp_ohe', 'exng_ohe', 'caa_ohe',\n",
        "                                        'thall_ohe','features_scaled'],\n",
        "                             outputCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247fdb95",
      "metadata": {
        "id": "247fdb95"
      },
      "outputs": [],
      "source": [
        "# Create stages list\n",
        "myStages = [assembler1, scaler, ohe, assembler2,lr]\n",
        "\n",
        "# Set up the pipeline\n",
        "pipeline = Pipeline(stages= myStages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab63f90",
      "metadata": {
        "id": "8ab63f90",
        "outputId": "94ad6d95-a87e-48a3-d22b-b4f04f4175ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+\n",
            "|label|         probability|prediction|\n",
            "+-----+--------------------+----------+\n",
            "|    1|[0.02082049276438...|       1.0|\n",
            "|    1|[0.00349694524882...|       1.0|\n",
            "|    1|[0.02497413140479...|       1.0|\n",
            "|    0|[0.68654443009022...|       0.0|\n",
            "|    1|[0.01359450880502...|       1.0|\n",
            "|    1|[0.00329226868052...|       1.0|\n",
            "|    1|[0.04984709511694...|       1.0|\n",
            "|    0|[0.67075368688499...|       0.0|\n",
            "|    1|[0.01929114560670...|       1.0|\n",
            "|    0|[0.69930120117003...|       0.0|\n",
            "|    1|[0.02657089565818...|       1.0|\n",
            "|    0|[0.94457287311715...|       0.0|\n",
            "|    1|[0.01897009099017...|       1.0|\n",
            "|    1|[0.00392892014022...|       1.0|\n",
            "|    1|[0.01147819212974...|       1.0|\n",
            "|    0|[0.33364414059559...|       1.0|\n",
            "|    1|[0.01173781003523...|       1.0|\n",
            "|    1|[0.01131678038432...|       1.0|\n",
            "|    1|[0.09699872384288...|       1.0|\n",
            "|    1|[0.00868910471995...|       1.0|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We fit the model using the training data.\n",
        "pModel = pipeline.fit(trainDF)\n",
        "\n",
        "# We transform the data.\n",
        "trainingPred = pModel.transform(trainDF)\n",
        "\n",
        "# # We select the actual label, probability and predictions\n",
        "trainingPred.select('label','probability','prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pModel.save(\"/pipelines\")"
      ],
      "metadata": {
        "id": "SwwqbzZr37Iw"
      },
      "id": "SwwqbzZr37Iw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3391605d",
      "metadata": {
        "id": "3391605d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "af96b9d4-221c-4893-bb8b-32096e7941dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory Contents: []\n",
            "  adding: content/pipelines_alternative/ (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_abb3d58d-2cba-48c8-a1f8-30a8ce9684a1\", \"pipelines_alternative.zip\", 210)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = \"/content/pipelines_alternative\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "# List the contents of the directory\n",
        "directory_contents = os.listdir(directory_path)\n",
        "print(\"Directory Contents:\", directory_contents)\n",
        "\n",
        "# Download the entire directory as a zip file\n",
        "zip_file_path = \"/content/pipelines_alternative.zip\"\n",
        "!zip -r $zip_file_path $directory_path\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "\n",
        "pModel = PipelineModel.load(\"/pipelines\")"
      ],
      "metadata": {
        "id": "IYgMZKLnnW_c"
      },
      "id": "IYgMZKLnnW_c",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation: pModel.transform(trainDF) applies the previously trained pipeline model pModel to the training DataFrame trainDF. The transformation involves applying each stage of the pipeline, including feature transformations and the final classification model, to the input data.\n",
        "\n",
        "Selection of Columns: trainingPred.select('label','probability','prediction') selects specific columns from the resulting DataFrame (trainingPred). The selected columns are:\n",
        "\n",
        "'label': The actual label or target variable from the training data.\n",
        "'probability': The probability distribution of the predicted classes. This is often used in classification problems to see the likelihood of each class.\n",
        "'prediction': The predicted class label based on the model."
      ],
      "metadata": {
        "id": "D8c5FYqWwSYt"
      },
      "id": "D8c5FYqWwSYt"
    },
    {
      "cell_type": "code",
      "source": [
        "# We transform the data.\n",
        "trainingPred = pModel.transform(trainDF)\n",
        "\n",
        "# # We select the actual label, probability and predictions\n",
        "trainingPred.select('label','probability','prediction').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkuBd6poBX5",
        "outputId": "8fdc03c1-3dd0-40df-e7b7-029c73808430"
      },
      "id": "UMkuBd6poBX5",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+\n",
            "|label|         probability|prediction|\n",
            "+-----+--------------------+----------+\n",
            "|    1|[0.02082049276438...|       1.0|\n",
            "|    1|[0.00349694524882...|       1.0|\n",
            "|    1|[0.02497413140479...|       1.0|\n",
            "|    0|[0.68654443009022...|       0.0|\n",
            "|    1|[0.01359450880502...|       1.0|\n",
            "|    1|[0.00329226868052...|       1.0|\n",
            "|    1|[0.04984709511694...|       1.0|\n",
            "|    0|[0.67075368688499...|       0.0|\n",
            "|    1|[0.01929114560670...|       1.0|\n",
            "|    0|[0.69930120117003...|       0.0|\n",
            "|    1|[0.02657089565818...|       1.0|\n",
            "|    0|[0.94457287311715...|       0.0|\n",
            "|    1|[0.01897009099017...|       1.0|\n",
            "|    1|[0.00392892014022...|       1.0|\n",
            "|    1|[0.01147819212974...|       1.0|\n",
            "|    0|[0.33364414059559...|       1.0|\n",
            "|    1|[0.01173781003523...|       1.0|\n",
            "|    1|[0.01131678038432...|       1.0|\n",
            "|    1|[0.09699872384288...|       1.0|\n",
            "|    1|[0.00868910471995...|       1.0|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partioning test data into 10 csv and maxfiles trigger 1"
      ],
      "metadata": {
        "id": "GkM30VKzvRzT"
      },
      "id": "GkM30VKzvRzT"
    },
    {
      "cell_type": "code",
      "source": [
        "testData = testDF.repartition(10)\n",
        "testData\n",
        "\n",
        "#Create a directory\n",
        "testData.write.format(\"CSV\").option(\"header\",False).save(\"/heart_streaming/\")"
      ],
      "metadata": {
        "id": "DNDBoDthooXD"
      },
      "id": "DNDBoDthooXD",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sourceStream = (\n",
        "    spark.readStream.schema(schema)\n",
        "    .option(\"maxFilesPerTrigger\", 1)\n",
        "    .csv(\"/heart_streaming\")\n",
        ")"
      ],
      "metadata": {
        "id": "kAT_o1HNorUy"
      },
      "id": "kAT_o1HNorUy",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prediction1 = pModel.transform(sourceStream).select('target',\n",
        "                                                   'probability',\n",
        "                                                   'prediction')\n"
      ],
      "metadata": {
        "id": "x5L5z9MMqaIk"
      },
      "id": "x5L5z9MMqaIk",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(prediction1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ndnSqKYBsHjL",
        "outputId": "ad15b66a-f6ef-4196-94e5-ce0b80591577"
      },
      "id": "ndnSqKYBsHjL",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[target: bigint, probability: vector, prediction: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the prediction of console"
      ],
      "metadata": {
        "id": "dkUpLYiJsegP"
      },
      "id": "dkUpLYiJsegP"
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = prediction1.writeStream.queryName(\"prediction1\") \\\n",
        "            .format(\"console\")\\\n",
        "            .trigger(once=True)\\\n",
        "            .start()\\\n",
        "            .awaitTermination()"
      ],
      "metadata": {
        "id": "xugFDHJzslv_"
      },
      "id": "xugFDHJzslv_",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = (\n",
        "            prediction1.writeStream.queryName(\"prediction4\")\n",
        "            .format(\"memory\")\n",
        "            .outputMode(\"append\")\n",
        "            .start())"
      ],
      "metadata": {
        "id": "fdm5XvoXs1fu"
      },
      "id": "fdm5XvoXs1fu",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(2):\n",
        "    df = spark.sql(\n",
        "        \"SELECT * FROM prediction4\")\n",
        "    df.show(10)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xMq71q9tA9O",
        "outputId": "e70d2c68-48dc-401f-d747-16085242b4c2"
      },
      "id": "8xMq71q9tA9O",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+\n",
            "|target|         probability|prediction|\n",
            "+------+--------------------+----------+\n",
            "|     0|[0.97397927826711...|       0.0|\n",
            "|     0|[0.94116965839550...|       0.0|\n",
            "|     1|[0.09057169258915...|       1.0|\n",
            "|     1|[0.00895484866126...|       1.0|\n",
            "|     0|[0.48344847711679...|       1.0|\n",
            "|     0|[0.87733369531352...|       0.0|\n",
            "|     0|[0.38316072658009...|       1.0|\n",
            "|     0|[0.96788920228249...|       0.0|\n",
            "|     0|[0.90723095682524...|       0.0|\n",
            "|     0|[0.55792532929343...|       0.0|\n",
            "+------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------+--------------------+----------+\n",
            "|target|         probability|prediction|\n",
            "+------+--------------------+----------+\n",
            "|     0|[0.97397927826711...|       0.0|\n",
            "|     0|[0.94116965839550...|       0.0|\n",
            "|     1|[0.09057169258915...|       1.0|\n",
            "|     1|[0.00895484866126...|       1.0|\n",
            "|     0|[0.48344847711679...|       1.0|\n",
            "|     0|[0.87733369531352...|       0.0|\n",
            "|     0|[0.38316072658009...|       1.0|\n",
            "|     0|[0.96788920228249...|       0.0|\n",
            "|     0|[0.90723095682524...|       0.0|\n",
            "|     0|[0.55792532929343...|       0.0|\n",
            "+------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[target: bigint, probability: vector, prediction: double]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.streams.active[0].isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9avpJFiRtMsw",
        "outputId": "4deface1-6dad-418a-c3a6-1e8f0dc7232f"
      },
      "id": "9avpJFiRtMsw",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.streams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA4rYFK3tQyB",
        "outputId": "80a49d93-9d71-4bd7-e375-cb575604fab6"
      },
      "id": "GA4rYFK3tQyB",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQueryManager at 0x78bedbc0a020>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query2.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOoU9D1WtVEE",
        "outputId": "2d9f237b-033b-4fb4-992d-65b0afe237ee"
      },
      "id": "kOoU9D1WtVEE",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Waiting for data to arrive',\n",
              " 'isDataAvailable': False,\n",
              " 'isTriggerActive': False}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}